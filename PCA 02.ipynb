{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74655648-8dab-485a-b6ce-6a919add60b0",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #C0AFDD; color: black; text-align: center; padding: 10px; border-radius: 10px; \">\n",
    "    <h1>PCA 02 </h1>\n",
    "    <h2> (Principal Component Analysis) </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aacfed-ddf6-4edc-a16f-e9477fe60f53",
   "metadata": {},
   "source": [
    "# Q1. What is a projection and how is it used in PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def24c52-234a-4b55-b8b1-960c08a517eb",
   "metadata": {},
   "source": [
    "PCA is a dimensionality reduction technique in the machine learning, which is use to capture the most important information from the dataset by identifying the most important featueres of the dataset. \n",
    "and projection is a technique in PCA, which reduces the dimensions of the data. A projection genrally refers to the transformation of data points from their original high-dimensional space to a lower-dimensional space defined by the principal components. This is achieved by multiplying the original data matrix by the transpose of the matrix of selected principal components.\n",
    "\n",
    "  `projection of x = x.v`\n",
    "  \n",
    "- where x is a new given datapoint\n",
    "- v is the matrix of principle component\n",
    "\n",
    "This equation gives the set of coordinates representing the data points in the lower-dimensional space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bd8cca-df8a-4c4c-9abe-924676151a3d",
   "metadata": {},
   "source": [
    "# Q2. How does the optimization problem in PCA work, and what is it trying to achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c571cb-5750-4093-ada2-e856a981e0ce",
   "metadata": {},
   "source": [
    "In the PCA our main objective is to reduce the dimensionality of the dataset while retaining as much information as possible. For getting our goal we need to find the optimal set of the vectors that capture the maximum varience in the data. so the information can retained as possible. And this has to be done by the optimixation. The steps of optimization shown below ;\n",
    "\n",
    "**Coverience Matrix :** covariance matrix summarizes the relationships and variances between different features in the dataset.\n",
    "\n",
    "**Eigenvalue and Eigenvector Computation:** The eigenvalues represent the amount of variance in the data along the corresponding eigenvectors. Larger eigenvalues indicate directions in which the data varies the most.\n",
    "\n",
    "**Selction of principle component :** The eigenvalues and eigenvectors are typically sorted in descending order based on the magnitude of the eigenvalues. The principal components are then selected from the top eigenvectors. These principal components form a new basis for the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2081d4e2-4128-4c54-9c6a-5fde8bbb9921",
   "metadata": {},
   "source": [
    "# Q3. What is the relationship between covariance matrices and PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722dfaf8-e335-4f97-9d31-1098e33c15b3",
   "metadata": {},
   "source": [
    "The principal components in PCA are derived from the covariance matrix of the original data. The eigenvectors of the covariance matrix represent the directions in which the data varies the most, and the corresponding eigenvalues indicate the magnitude of the variance along those directions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c9f410-edbc-40ae-8168-929188a89393",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bbc94c-1916-4858-af6c-f9916024c8c4",
   "metadata": {},
   "source": [
    "# Q4. How does the choice of number of principal components impact the performance of PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708eca69-f16e-40f1-ab75-84d426117d93",
   "metadata": {},
   "source": [
    "Number of principal components are putting high impact the performance of PCA. It involves a trade-off between dimensionality reduction and the amount of information retained. Here some factors are mentioned ;\n",
    "\n",
    "1. **Underfitting and Overfitting :**\n",
    "Selecting the few number of diemnsion may lead to underfitting as some of the information may be lost or uncaptured in the process . While using the high number of the dimensions may lead to the overfitting as models captures the extra noise from the training data and become too trained with the data.\n",
    "\n",
    "2. **Explained Variance:**\n",
    "The number of principal components determines how much variance in the original data is retained in the reduced-dimensional space. Each principal component is associated with an eigenvalue, and the sum of the eigenvalues represents the total variance in the data. By selecting a subset of principal components and calculating the proportion of the total variance they explain, we can assess how much information is retained.\n",
    "\n",
    "3. **Computetional Cost :**\n",
    "Using the fewer dimensions of the data may lead to the good processing as computetional complexity is very low but it's important to balance computational efficiency with the need to retain sufficient information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea364447-5a74-4874-8e58-e66fecded2de",
   "metadata": {},
   "source": [
    "# Q5. How can PCA be used in feature selection, and what are the benefits of using it for this purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04af0333-765f-4fc2-85fc-069132ffd38f",
   "metadata": {},
   "source": [
    "Feature Selection is a technique for selecting the important features from the dataset and ignore the less important fratures of dataset , for predicting the output. As a default system gives the equal weightage to all the features wether it is highly co- related or low- correlated to the dependent feature. So the performance of the model will decrese. In such a situation feature selction helps us as it gives the weightage to the features of the datasets as they deserve it.\n",
    "\n",
    "While PCA is a dimensionality reduction technique which is not selecting the features as the feature selection but doing a simillar kind of work as feature selection. As PCA is just melting the large number of the dimensions in the lesser number of dimensions. In this process the somwhow the information information of the features, even which are less important also captured and feature selection also has to be done because the number of features are reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12136a0-1232-4192-a201-ffb5d6d245c5",
   "metadata": {},
   "source": [
    "# Q6. What are some common applications of PCA in data science and machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddbcb75-7ad0-4de4-b414-a554e0adb762",
   "metadata": {},
   "source": [
    "PCA is used in various fields of meacine learning in the datascience.Which are shown below ;\n",
    "\n",
    "**Dimensionality Reduction :** Main objective of the principle component alalysis is to reduce the number of dimensions in the data . so the model will become  more efficient and faster.\n",
    "\n",
    "**Face Recognition:** PCA is applied in facial recognition systems to extract the most relevant features from facial images. By representing faces in a lower-dimensional space, it becomes easier to compare and recognize faces.\n",
    "\n",
    "**Speech Recognition:** PCA is employed in speech processing to reduce the dimensionality of features extracted from audio signals. This aids in improving the efficiency of speech recognition algorithms.\n",
    "\n",
    "**Data Visualization:** PCA is often utilized for visualizing high-dimensional data in a lower-dimensional space (e.g., 2D or 3D). This aids in exploratory data analysis and can reveal underlying patterns or clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c8a98b-041e-4206-8e2c-abd8c49ddbd8",
   "metadata": {},
   "source": [
    "# Q7.What is the relationship between spread and variance in PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163c25aa-542b-4d41-a380-dfc68f5c893b",
   "metadata": {},
   "source": [
    "In PCA, the spread of the data along each principal component is represented by the variance. The principal components are the directions in which the data varies the most. The first principal component corresponds to the direction of maximum variance, the second principal component corresponds to the direction of the second highest variance, and so on.\n",
    "\n",
    "Variance is a measure of the spread or dispersion of a set of data points. In PCA, the variance along each principal component reflects the spread of the data along that direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dded58e-92ca-406c-aa38-104389ce7d4d",
   "metadata": {},
   "source": [
    "# Q8. How does PCA use the spread and variance of the data to identify principal components?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc18b73-75b9-48bd-bc75-9b2a2585c52a",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) identifies principal components by finding the directions in the data with the maximum variance. The steps involved in using spread and variance to identify principal components are as following steps:\n",
    "\n",
    "- 1.Covariance Matrix Calculation\n",
    "- 2.Eigenvalue and Eigenvector Computation\n",
    "- 3.Ranking Eigenvectors by Eigenvalues\n",
    "- 4.Selecting Principal Components\n",
    "- 5.Projecting Data onto Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc884f8-77ff-4b69-b242-fdff58acc66b",
   "metadata": {},
   "source": [
    "# Q9. How does PCA handle data with high variance in some dimensions but low variance in others?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d03fc87-5fa5-4317-8768-e41ba69e09b6",
   "metadata": {},
   "source": [
    "PCA tackles data with varying variances across dimensions by identifying the axes of maximum variance, known as principal components. It sifts through the multidimensional space, pinpointing the directions where the data varies the most. This makes PCA adept at handling situations where some dimensions exhibit high variability while others remain subdued. By focusing on the high-variance dimensions, PCA distills the essence of the dataset into a reduced set of components, preserving the most impactful features and aiding in noise reduction. The resulting representation emphasizes the dimensions that contribute significantly to the overall data variability, allowing for more efficient analysis and visualization.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
